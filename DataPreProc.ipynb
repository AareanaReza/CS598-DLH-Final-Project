{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AareanaReza/CS598-DLH-Final-Project/blob/main/DataPreProc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "YqAvWzmk_R-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m2lVXKITeAF",
        "outputId": "983d0985-191d-4b0d-bf53-adcc63951a7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Git "
      ],
      "metadata": {
        "id": "lpvUXWB7CUP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd drive/MyDrive/CS5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT7cV1UTCZsr",
        "outputId": "91fa7bd8-9914-4240-d066-07d49e5e9ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and only do once\n",
        "#! git clone https://github.com/AareanaReza/CS598-DLH-Final-Project.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmifOJXNCtgO",
        "outputId": "b423ee13-8807-4c91-9493-15182d9251b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CS598-DLH-Final-Project'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 609 bytes | 17.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# always pull before working on code\n",
        "#! git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trC6VJnyDC6C",
        "outputId": "456be98d-c68e-4575-e5ed-252e62800c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS598-DLH-Final-Project/Colab-Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hcEDi8LLJX_",
        "outputId": "6bd47956-604d-4a8c-e219-86fe310d23a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS598-DLH-Final-Project/Colab-Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "pGLPPQgE_O5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gzip\n",
        "import csv\n",
        "from itertools import islice\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import sys\n",
        "import os\n"
      ],
      "metadata": {
        "id": "CtL6i0l3_NFO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path Variables"
      ],
      "metadata": {
        "id": "9XXDnvd5_gkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PREPROCESSING_PATH = '/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/'\n",
        "NOTEEVENTS_CSV_GZ = DATA_PREPROCESSING_PATH + 'Original-Data/NOTEEVENTS.csv.gz'\n",
        "outpath = DATA_PREPROCESSING_PATH + 'Output-Data/'\n"
      ],
      "metadata": {
        "id": "EyQyZP-e7oFN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform File to .csv and Look at Data"
      ],
      "metadata": {
        "id": "16dyVILO_tP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gzip.open(NOTEEVENTS_CSV_GZ, 'rt') as csv_file:\n",
        "    csv_data = csv_file.read()\n",
        "    with open( DATA_PREPROCESSING_PATH + 'Original-Data/NOTEEVENTS.csv', 'wt') as out_file:\n",
        "         out_file.write(csv_data)"
      ],
      "metadata": {
        "id": "CGPGAwSP7OKC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NOTEEVENTS_CSV = DATA_PREPROCESSING_PATH + 'Original-Data/NOTEEVENTS.csv'"
      ],
      "metadata": {
        "id": "Hu4pQQi8xFcg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(NOTEEVENTS_CSV, 'rt') as file:\n",
        "  test = file.read()"
      ],
      "metadata": {
        "id": "UfudwBtUz1mR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = csv.reader(test)\n",
        "lines= len(list(reader))\n",
        "print(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u0VZyjV0QMF",
        "outputId": "d518d188-315b-4c82-8edc-d111a897fc31"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148190369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader = csv.DictReader(test)\n",
        "for row in islice(reader, 100):\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "eX1Wffe90GdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = csv.DictReader(csv_data)\n",
        "for row in islice(reader, 100):\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "bqwImw_X82di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "S2S-30AYARvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dico for references\n",
        "ones = {\"1\": \"one\", \"2\": \"two\", \"3\": \"three\", \"4\": \"four\", \"5\": \"five\",\n",
        "        \"6\": \"six\", \"7\": \"seven\", \"8\": \"eight\", \"9\": \"nine\"}\n",
        "afterones = {\"10\": \"ten\", \"11\": \"eleven\", \"12\": \"twelve\", \"13\": \"thirteen\", \"14\": \"fourteen\", \"15\": \"fifteen\",\n",
        "             \"16\": \"sixteen\", \"17\": \"seventeen\", \"18\": \"eighteen\", \"19\": \"nineteen\"}\n",
        "tens = {\"2\": \"twenty\", \"3\": \"thirty\", \"4\": \"fourty\", \"5\": \"fifty\",\n",
        "        \"6\": \"sixty\", \"7\": \"seventy\", \"8\": \"eighty\", \"9\": \"ninety\"}\n",
        "grand = {0: \" billion \", 1: \" million \", 2: \" thousand \", 3: \"\"}\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def get_next_line_without_moving(f):\n",
        "    pos = f.tell()\n",
        "    line = f.readline()\n",
        "    line = f.readline()\n",
        "    f.seek(pos)\n",
        "    return line\n",
        "\n",
        "\n",
        "def get_vocabulary(inputfile):\n",
        "    \"\"\" This procedure takes a MIMIC NoteEvents file and returns a dictionary\n",
        "    which contains words and their corresponding count \"\"\"\n",
        "    # Ignore first line (columns title)\n",
        "    # If comma in the line, ignore it as it is NOT text\n",
        "    # Otherwise, take the line, and foreach word in line, if word in dict.keys(), count++, otherwise new words\n",
        "    word_dict = dict()\n",
        "    with open(inputfile) as fp:\n",
        "        # Ignore first line\n",
        "        line = fp.readline()\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if line == \"\\n\" or \",\" in line or \"\\\"\" in line:\n",
        "                continue\n",
        "            if not line:\n",
        "                break\n",
        "            word_list = word_tokenize(line)\n",
        "            for w in word_list :\n",
        "                if w in word_dict.keys():\n",
        "                    word_dict[w] += 1\n",
        "                else:\n",
        "                    word_dict[w] = 1\n",
        "    print(\"Vocabulary size:\", len(word_dict))\n",
        "    return word_dict\n",
        "\n",
        "\n",
        "def show_histogram(distribution, n_bins, title):\n",
        "    plt.style.use('ggplot')\n",
        "    plt.title(title)\n",
        "    plt.hist(distribution, bins=n_bins)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_paragraph_distribution(inputfile):\n",
        "    \"\"\" Displays the number of paragraph in the file for each size of character\"\"\"\n",
        "    # Array saving the length of paragraphs\n",
        "    par_lengths = []\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if line == \"\\n\":\n",
        "                continue\n",
        "            if not line:\n",
        "                break\n",
        "            par_lengths.append(len(line))\n",
        "    # Now we display the histograms\n",
        "    show_histogram(par_lengths, max(par_lengths), 'Number of paragraph with respect to its size')\n",
        "\n",
        "\n",
        "def replace_breakline_by_space(given_line, next_line):\n",
        "    \"\"\" Replaces '\\n' by ' ' at the end of the given line if exists\n",
        "    This function is called by paragraphFinder\n",
        "    \"\"\"\n",
        "    if len(given_line) == 0:\n",
        "        return given_line\n",
        "    if given_line.count('\"') > 0 :\n",
        "        return given_line\n",
        "    if next_line.count('\"') > 0 :\n",
        "        return given_line\n",
        "    if given_line[len(given_line)-1] != '\\n':\n",
        "        return given_line\n",
        "    given_line = given_line.replace(given_line[len(given_line)-1], ' ')\n",
        "    return given_line\n",
        "\n",
        "\n",
        "def three_dig_to_words(val):\n",
        "    \"\"\" Function converting number to words of 3 digit\n",
        "    Code from Barath Kumar\n",
        "    Link : https://stackoverflow.com/questions/15598083/python-convert-numbers-to-words\n",
        "    \"\"\"\n",
        "    if val != \"000\":\n",
        "        ans = \"\"\n",
        "        if val[0] in ones:\n",
        "            ans = ans + ones[val[0]] + \" hundred \"\n",
        "        if val[1:] in afterones:\n",
        "            ans = ans + afterones[val[1:]] + \" \"\n",
        "        elif val[1] in tens:\n",
        "            ans = ans + tens[val[1]] + \" \"\n",
        "        if val[2] in ones and val[1:] not in afterones:\n",
        "            ans = ans + ones[val[2]]\n",
        "        return ans\n",
        "\n",
        "\n",
        "def num_to_words(value):\n",
        "    \"\"\" This function takes an integer as an input, and outputs its text version\n",
        "    Works with integer from 0 to 999 999 999 999.\n",
        "    \"\"\"\n",
        "    # Padding with zeros\n",
        "    pad = 12 - len(str(value))\n",
        "    padded = \"0\" * pad + str(value)\n",
        "\n",
        "    # Exception case\n",
        "    if padded == \"000000000000\":\n",
        "        return \"zero\"\n",
        "\n",
        "    # Preparing the values before computation\n",
        "    result = \"\"\n",
        "    number_groups = [padded[0:3], padded[3:6], padded[6:9], padded[9:12]]\n",
        "\n",
        "    for key, val in enumerate(number_groups):\n",
        "        if val != \"000\":\n",
        "            result = result + three_dig_to_words(val) + grand[key]\n",
        "\n",
        "    result = re.sub(r'(^ *| *$)', ' ', result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "xuJyDnnq-oei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff249d5a-02c8-457e-a268-9ba146e36d28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def anonimization_remover(inputfile, outputfile):\n",
        "    \"\"\" Anonimization mark '[** **]' and its content are removed\"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            cleaned_line = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', line)\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "A79igKOxA9-D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anonimization_remover(NOTEEVENTS_CSV, outpath+'out_noanonim.csv')"
      ],
      "metadata": {
        "id": "vfcptbE8BWIe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def doctor_quotes_remover(inputfile, outputfile):\n",
        "    \"\"\" Doctor quotes mark \"\" \"\" are removed \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            cleaned_line = re.sub('\"\"', '', line)\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "prNRJdswp4zO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doctor_quotes_remover(outpath+'out_noanonim.csv', outpath+'out_nodocquotes.csv')\n",
        "os.remove(outpath+'out_noanonim.csv')"
      ],
      "metadata": {
        "id": "fCoGUzhEp6T7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shape_to_csv(inputfile, outputfile):\n",
        "    \"\"\" Ensures that we have a CSV shape : we separate the text columns from\n",
        "    the others columns with breaklines in the right places \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    count_comma = 0\n",
        "    within_text = False\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            if count_comma >= 10 or within_text:\n",
        "                if line.count(\"\\\"\") == 0:\n",
        "                    processed_file.write(line)\n",
        "                    continue\n",
        "            else:\n",
        "                if line.count(\",\") == 0:\n",
        "                    processed_file.write(line)\n",
        "                    continue\n",
        "            ## Some cases that could break the pipeline !\n",
        "            # if \"Resident Progress Note, CCU\" in line : # one additional comma to count\n",
        "            #     count_comma -= 1\n",
        "            # elif \"Code Blue, Cardiac Arrest, Death\" in line : # two add. comma to count\n",
        "            #     count_comma -= 2\n",
        "            # ...\n",
        "            ### START This part of code generalizes the previous example\n",
        "            cntcm = line.count(',')\n",
        "            cntqt = line.count('\"')\n",
        "            if cntqt != 0 and cntcm != 0 :\n",
        "                if cntcm != 10 :\n",
        "                    if cntqt > 1 :\n",
        "                        groups = line.split('\"')\n",
        "                        test = '\"'.join(groups[:cntqt]), '\"'.join(groups[cntqt:])\n",
        "                        if test[0].count(',') != 10 :\n",
        "                            count_comma -= test[0].count(',') - 10\n",
        "            ### END This part of code generalizes the previous example\n",
        "            index = -1\n",
        "            for c in line:\n",
        "                index += 1\n",
        "                if c == ',' and within_text == False:\n",
        "                    count_comma += 1\n",
        "                if c == '\\\"':\n",
        "                    if count_comma >= 10:\n",
        "                        count_comma = 0\n",
        "                        within_text = True\n",
        "                        line = line[:index+1] + '\\n' + line[index+1:]\n",
        "                        index += 1\n",
        "                    elif within_text:\n",
        "                        within_text = False\n",
        "                        line = line[:index+1] + '\\n' + line[index+1:]\n",
        "                        line = line[:index] + '\\n' + line[index:]\n",
        "                        index += 2\n",
        "            processed_file.write(line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "PgKnjR8nqoWp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape_to_csv(outpath+'out_nodocquotes.csv', outpath+'out_csvshape.csv')\n",
        "os.remove(outpath+'out_nodocquotes.csv')"
      ],
      "metadata": {
        "id": "oyYJxX0hrJOj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_commas(inputfile, outputfile):\n",
        "    \"\"\" Toss off commas that are within the Description field \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            commas = line.count(',')\n",
        "            if line.count(\"\\\"\") > 0 :\n",
        "                if (commas == 0 or commas == 10) :\n",
        "                    processed_file.write(line)\n",
        "                if commas > 10 :\n",
        "                    # remove_extra_commas\n",
        "                    cleaned_line = re.sub(r'\"[^\"]*\"', lambda m: m.group(0).replace(',', ''), line)\n",
        "                    processed_file.write(cleaned_line)\n",
        "            else :\n",
        "                processed_file.write(line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "oZUfSUlt4W5U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_extra_commas(outpath+'out_csvshape.csv', outpath+'out_noextracommas.csv')\n",
        "os.remove(outpath+'out_csvshape.csv')"
      ],
      "metadata": {
        "id": "W7X6Q3bB4lwq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_all_text(inputfile, outputfile):\n",
        "    \"\"\" Every letter in the text becomes lowercase \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            cleaned_line = line.lower()\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "PEy1yG215Tb9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_all_text(outpath+'out_noextracommas.csv', outpath+'out_lower.csv')\n",
        "os.remove(outpath+'out_noextracommas.csv')"
      ],
      "metadata": {
        "id": "1HJEemoa5UkJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_useless_words(inputfile, outputfile):\n",
        "    \"\"\"\n",
        "    1) This procedure removes the useless part of the first line of the text : \"Admission Date: \" \"Discharge Date: \"\n",
        "    2) It removes the useless words (DATE OF BIRTH, SERVICE, SEX, ADDENDUM)\n",
        "    3) At the end of many texts, there are some useless infos such as (JOB#, D:, T:, Dictated by:,...)\n",
        "    4) Removes the days of a week\n",
        "    \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            # 1st task\n",
        "            cleaned_line = re.sub(r'admission date:.*', '', line)\n",
        "            # 2nd task\n",
        "            cleaned_line = re.sub(r'sex *?: *[mf]?[ \\n]', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'date of birth *?:', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'service *?: *?.*?\\n$', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'addendum *?:?', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'medquist36', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'm\\.d\\.', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'\\Wmd\\W|^md\\W', '', cleaned_line)\n",
        "            # 3rd task\n",
        "            cleaned_line = re.sub(r'^dictated *?by *?: *', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'^completed *?by *?: *', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'^cc *?(by)? *?: *', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'^d *?: *(\\d\\d:\\d\\d)?', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'^t *?: *(\\d\\d:\\d\\d)?', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'phone *?: *', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'provider *?: *', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'date/time *?: *', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'^job# *?: *', '', cleaned_line)\n",
        "            # 4th task\n",
        "            cleaned_line = re.sub(r'monday', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'tuesday', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'wednesday', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'thursday', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'friday', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'saturday', '', cleaned_line)\n",
        "            cleaned_line = re.sub(r'sunday', '', cleaned_line)\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "pmElA-si562A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_useless_words(outpath+'out_lower.csv', outpath+'out_nobadwords.csv')\n",
        "os.remove(outpath+'out_lower.csv')"
      ],
      "metadata": {
        "id": "gXV5QSdv6MJl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_remover(inputfile, outputfile):\n",
        "    \"\"\" Remove time present in the file\n",
        "    HH:MM information are completely irrelevant in the eyes of the machine because it does not have any influence\n",
        "    on patient's health, future diagnoses, and so on. So we remove them.\n",
        "    \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            if line.count(\"\\\"\") > 0:\n",
        "                processed_file.write(line)\n",
        "                continue\n",
        "            cleaned_line = re.sub(r'\\d?\\d:\\d\\d *?((am|pm)\\W)?', '', line)\n",
        "            cleaned_line = re.sub(r'\\d?\\d:\\d\\d:\\d\\d *?((am|pm)\\W)?', '', cleaned_line)\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "nAJfy1537iHw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_remover(outpath+'out_nobadwords.csv', outpath+'out_notime.csv')\n",
        "os.remove(outpath+'out_nobadwords.csv')"
      ],
      "metadata": {
        "id": "gHTTKJvg7obM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repetitive_number_parentheses(inputfile, outputfile):\n",
        "    \"\"\" Doctors write a lot numbers in letters followed by the actual number within parenthesis, like that :\n",
        "    << He should take one (1) at bedtime and two (2) in the morning. >>\n",
        "    We remove the parenthesis part\n",
        "    \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            cleaned_line = re.sub(r'\\(\\d\\) ?', '', line)\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "gEX5Z2As762z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repetitive_number_parentheses(outpath+'out_notime.csv', outpath+'out_noparentheses.csv')\n",
        "os.remove(outpath+'out_notime.csv')"
      ],
      "metadata": {
        "id": "BplkfLjC793m"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spaces_remover(inputfile, outputfile):\n",
        "    \"\"\" This procedure does two things :\n",
        "    1) It removes all spaces that are starting a paragraph (a line)\n",
        "    2) It replaces every \"more than 1 space in a row\" by 1 space\n",
        "    \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            # If paragraph starts with spaces, remove all of them\n",
        "            cleaned_line = re.sub(r'^ +', '', line)\n",
        "            # If more than two spaces within paragraph : leave it with two spaces\n",
        "            cleaned_line = re.sub(r' +', ' ', cleaned_line)\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()\n",
        "\n",
        "def paragraph_finder(inputfile, outputfile):\n",
        "    \"\"\" This function does the following :\n",
        "    1) If we find a case where we have \"{anything}\\n([NUMBER].|#){anything}\", we recognize that there is a new paragraph\n",
        "    2) If we find two or more occurrences of '\\n' in a row, we keep at least one occurrence (new paragraph)\n",
        "    3) Otherwise, if the two previous rules don't apply, we replace '\\n' by a ' ' as we think we are within a paragraph.\n",
        "    \"\"\"\n",
        "    # REGEX : if the string contains a number followed by a dot, it is the start of a new paragraph\n",
        "    reg_exp_new_line = re.compile(r'^[1-9][0-9]?\\. +|^#')\n",
        "    # REGEX : if a line match this, consider this line as an empty line\n",
        "    bad_line_re = re.compile(r'^[,.]* *\\n$')\n",
        "    # BOOL : indicate if we had an empty line before (in this case, the next line starts a new paragraph for sure)\n",
        "    previous_was_empty = 0\n",
        "    # Prepare the output file\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    paragraph = \"\"\n",
        "    with open(inputfile) as fp:\n",
        "        line = fp.readline()\n",
        "        paragraph += line\n",
        "        while line:\n",
        "            line = fp.readline()\n",
        "            # if line != \"\\n\" and line != \".\\n\":\n",
        "            if line.count(\"\\\"\")>0 :\n",
        "                if line.count(\"\\\"\") == 1:\n",
        "                    line = '\\n'+line\n",
        "                    processed_file.write(paragraph)\n",
        "                    paragraph = line\n",
        "                    continue\n",
        "            if not bad_line_re.match(line):\n",
        "                if previous_was_empty:\n",
        "                    # Save paragraph, now we have a new one\n",
        "                    paragraph += \"\\n\"\n",
        "                    processed_file.write(paragraph)\n",
        "                    line = replace_breakline_by_space(line, get_next_line_without_moving(fp))\n",
        "                    paragraph = line\n",
        "                elif reg_exp_new_line.match(line):\n",
        "                    # Save paragraph, now we have a new one\n",
        "                    paragraph += \"\\n\"\n",
        "                    processed_file.write(paragraph)\n",
        "                    line = replace_breakline_by_space(line, get_next_line_without_moving(fp))\n",
        "                    paragraph = line\n",
        "                else:\n",
        "                    line = replace_breakline_by_space(line, get_next_line_without_moving(fp))\n",
        "                    paragraph += line\n",
        "                previous_was_empty = 0\n",
        "            else:\n",
        "                previous_was_empty = 1\n",
        "    processed_file.close()\n",
        "\n",
        "def preprocess_enumerations(inputfile, outputfile):\n",
        "    \"\"\" Recognize if a numerated list is a list of paragraph or a list of elements\n",
        "    We do so by implementing thresholds : max # char and avg # char over all lines the list\n",
        "    In the meantime, we also remove the digits. and # part of the lines\n",
        "    \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    regex_start_enum = re.compile(r'^[1-9][0-9]?\\. +|^#')\n",
        "    start_of_enum = False\n",
        "    current_list = \"\"\n",
        "    len_list = []\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                if start_of_enum:\n",
        "                    # write the last\n",
        "                    if max(len_list) < 300:\n",
        "                        if sum(len_list) / len(len_list) < 250:\n",
        "                            current_list = re.sub(r'\\n', ' ', current_list)\n",
        "                    else:\n",
        "                        if sum(len_list) / len(len_list) < 150:\n",
        "                            current_list = re.sub(r'\\n', ' ', current_list)\n",
        "                    processed_file.write(current_list)\n",
        "                    start_of_enum = False # do not forget to disable it\n",
        "                break\n",
        "            if regex_start_enum.match(line) and line.count('\"')==0:\n",
        "                cleaned_line = re.sub(r'^[1-9][0-9]?\\. +|^#+ *', '', line)\n",
        "                current_list += cleaned_line\n",
        "                len_list.append(len(cleaned_line))\n",
        "                start_of_enum = True\n",
        "            else:\n",
        "                if start_of_enum:\n",
        "                    # Uncomment the three following lines to see the max and avg values for each list\n",
        "                    # print(current_list)\n",
        "                    # print(\"Average : \" + str(sum(len_list)/len(len_list)))\n",
        "                    # print(\"Max : \" + str(max(len_list)))\n",
        "                    # Now that we have the needed info of the list, we clean it\n",
        "                    if max(len_list) < 300:\n",
        "                        if sum(len_list)/len(len_list) < 250:\n",
        "                            current_list = re.sub(r'\\n', ' ', current_list)\n",
        "                    else:\n",
        "                        if sum(len_list) / len(len_list) < 150:\n",
        "                            current_list = re.sub(r'\\n', ' ', current_list)\n",
        "                    processed_file.write(current_list)\n",
        "                    current_list = \"\"\n",
        "                    start_of_enum = False\n",
        "                    len_list = []\n",
        "\n",
        "                if line.count('\"')!= 0:\n",
        "                    line = '\\n' + line\n",
        "                processed_file.write(line)\n",
        "    processed_file.close()\n",
        "\n",
        "def toss_off_rare_words(inputfile, outputfile, word_dict):\n",
        "    \"\"\" Toss off words that occur less than 5 times in the corpus \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    a_subset = {key: value for key, value in word_dict.items() if value < 5}\n",
        "    print(\"Tossing off rare words.\\nSize of words with less than 5 frequency:\", len(a_subset))\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if line == \"\\n\" or \",\" in line or \"\\\"\" in line:\n",
        "                processed_file.write(line)\n",
        "                continue\n",
        "            if not line:\n",
        "                break\n",
        "            word_list = word_tokenize(line)\n",
        "            for w in word_list :\n",
        "                if w in a_subset.keys():\n",
        "                    re.sub(w, '', line)\n",
        "            processed_file.write(line)\n",
        "    processed_file.close()\n",
        "    # If you want to see the tossed off words, uncomment the next line\n",
        "    # print(a_subset.items())\n",
        "\n",
        "def numbers_to_text(inputfile, outputfile):\n",
        "    \"\"\" Transform numbers to their textual version - word2vec performs better with textual info\n",
        "        WARNING : do not convert IDs into textual, only do that to numbers inside the TEXT cells.\n",
        "    \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "\n",
        "            # Avoid transforming IDs to letters (it doesn't make sense at all)\n",
        "            count_comma = line.count(',')\n",
        "            count_quote = line.count('\"')\n",
        "            if count_comma >= 10 and count_quote >= 1:\n",
        "                processed_file.write(line)\n",
        "                continue\n",
        "\n",
        "            # if \\d+\\.\\d+ is found, transform . to [space]point[space]\n",
        "            # (and if there are zeros after the dot, replace them by \"zero \")\n",
        "            cleaned_line = re.sub(r'((\\d|)*)\\.00(\\d+)', r' \\1 point zero zero \\3 ', line)\n",
        "            cleaned_line = re.sub(r'((\\d|)*)\\.0(\\d+)', r' \\1 point zero \\3 ', cleaned_line)\n",
        "            cleaned_line = re.sub(r'((\\d|)*)\\.(\\d+)', r' \\1 point \\3 ', cleaned_line)\n",
        "            # for all digits found, replace it by the text form (sub)\n",
        "            cleaned_line = re.sub(r'([1-9]\\d*|0)', lambda x: num_to_words(x.group()), cleaned_line)\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()\n",
        "\n",
        "def special_char_remover(inputfile, outputfile):\n",
        "    \"\"\" Removes special chars : they are irrelevant - word2vec doesn't like that \"\"\"\n",
        "    processed_file = open(outputfile, 'w')\n",
        "    with open(inputfile) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line:\n",
        "                break\n",
        "\n",
        "            # Avoid removing special chars in the IDs row\n",
        "            count_comma = line.count(',')\n",
        "            count_quote = line.count('\"')\n",
        "            if count_comma >= 10 and count_quote >= 1:\n",
        "                processed_file.write(line)\n",
        "                continue\n",
        "\n",
        "            cleaned_line = re.sub(r'[*<>!?#.^;$&~_/\\\\]', '', line)\n",
        "            cleaned_line = re.sub(r'[-+=():,\\']', ' ', cleaned_line)\n",
        "            cleaned_line = re.sub(r'\\[|\\]', ' ', cleaned_line)\n",
        "            cleaned_line = re.sub(r'\\w%', ' percent', cleaned_line)\n",
        "            cleaned_line = re.sub(r'%', 'percent', cleaned_line)\n",
        "            processed_file.write(cleaned_line)\n",
        "    processed_file.close()"
      ],
      "metadata": {
        "id": "O16wlxebXU9X"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spaces_remover(outpath+'out_noparentheses.csv', outpath+'out_nospaces.csv')\n",
        "os.remove(outpath+'out_noparentheses.csv')\n",
        "paragraph_finder(outpath+'out_nospaces.csv', outpath+'out_paragraphs.csv')\n",
        "os.remove(outpath+'out_nospaces.csv')\n",
        "preprocess_enumerations(outpath+'out_paragraphs.csv', outpath+'out_enum.csv')\n",
        "os.remove(outpath+'out_paragraphs.csv')\n",
        "numbers_to_text(outpath+'out_enum.csv', outpath+'out_nonumbers.csv')\n",
        "os.remove(outpath+'out_enum.csv')\n",
        "special_char_remover(outpath+'out_nonumbers.csv', outpath+'out_nospecchar.csv')\n",
        "os.remove(outpath+'out_nonumbers.csv')\n",
        "spaces_remover(outpath+'out_nospecchar.csv', outpath+'out_nospaces2.csv')\n",
        "os.remove(outpath+'out_nospecchar.csv')\n",
        "word_dico = get_vocabulary(outpath+'out_nospaces2.csv')\n",
        "toss_off_rare_words(outpath+'out_nospaces2.csv', outpath+'out_norare.csv', word_dico)\n",
        "os.remove(outpath+'out_nospaces2.csv')\n",
        "spaces_remover(outpath+'out_norare.csv', outpath+'output.csv')\n",
        "os.remove(outpath+'out_norare.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0wph0JwYvWV",
        "outputId": "c5f8a719-6c81-4fa2-adfc-1eef728b6562"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 743079\n",
            "Tossing off rare words.\n",
            "Size of words with less than 5 frequency: 583330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/Output-Data/output.csv') as csvfile:\n",
        "    csv_data2 = csvfile.read()\n",
        "\n",
        "reader = csv.DictReader(csv_data2)\n",
        "for row in islice(reader, 100):\n",
        "  print(row)"
      ],
      "metadata": {
        "id": "O0RBpks_eghP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}