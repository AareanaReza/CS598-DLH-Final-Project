{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AareanaReza/CS598-DLH-Final-Project/blob/main/CS598ProjectCodeCommands.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CS 598 DLH Project: Diagnoses Prediction"
      ],
      "metadata": {
        "id": "RsGBXgDxSiTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive"
      ],
      "metadata": {
        "id": "YqAvWzmk_R-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0m2lVXKITeAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Paper and Github Citations"
      ],
      "metadata": {
        "id": "SshvD8HXQAlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zaghir J, Rodrigues-Jr JF, Goeuriot L, Amer-Yahia S. Real-world Patient Trajectory Prediction from Clinical Notes Using Artificial Neural Networks and UMLS-Based Extraction of Concepts. J Healthc Inform Res. 2021 Jun 5;5(4):474-496. doi: 10.1007/s41666-021-00100-z. PMID: 35419508; PMCID: PMC8982755.\n",
        "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8982755/#CR22\n",
        "GitHub:https://github.com/JamilProg/patient_trajectory_prediction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qRxiG2HKSKWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dependencies"
      ],
      "metadata": {
        "id": "pGLPPQgE_O5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import csv\n",
        "from itertools import islice\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import pickle\n",
        "import math\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as dt\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.metrics import roc_auc_score as roc\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ],
      "metadata": {
        "id": "CtL6i0l3_NFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "i21lbS5NcZgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Data Download Instructions\n",
        "\n"
      ],
      "metadata": {
        "id": "a1YKA57AQEkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download NOTEEVENTS.csv, ADMISSION.csv, and DIAGNOSES_ICD.csv you will need to request access and download the files from this link:https://physionet.org/content/mimiciii/1.4/"
      ],
      "metadata": {
        "id": "jGZE8NxBaxJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will aslo need to install QuickUMLS which require file access from the National Library of Medicine, see instructions : https://github.com/Georgetown-IR-Lab/QuickUMLS"
      ],
      "metadata": {
        "id": "q8faSLQEbQ3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing Code"
      ],
      "metadata": {
        "id": "u_tZ4N5XR_2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path Variables"
      ],
      "metadata": {
        "id": "9XXDnvd5_gkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PREPROCESSING_PATH = '/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/'\n",
        "NOTEEVENTS_CSV_GZ = DATA_PREPROCESSING_PATH + 'Original-Data/NOTEEVENTS.csv.gz'\n",
        "outpath = DATA_PREPROCESSING_PATH + 'Output-Data/'\n"
      ],
      "metadata": {
        "id": "EyQyZP-e7oFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NOTEEVENTS_CSV = DATA_PREPROCESSING_PATH + 'Original-Data/NOTEEVENTS.csv'"
      ],
      "metadata": {
        "id": "Hu4pQQi8xFcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADMISSIONS_CSV = DATA_PREPROCESSING_PATH + 'Original-Data/ADMISSIONS.csv'"
      ],
      "metadata": {
        "id": "sueJgCCy_5hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIAGNOSES_ICD = DATA_PREPROCESSING_PATH + 'Original-Data/DIAGNOSES_ICD.csv'"
      ],
      "metadata": {
        "id": "bLZiQPBuKELx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of Data Statistics"
      ],
      "metadata": {
        "id": "VjqgSeOCc2XH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes Events Data"
      ],
      "metadata": {
        "id": "8mEo6mrF6MCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set size and color for plots\n",
        "sns.reset_defaults()\n",
        "sns.set(\n",
        "    rc={'figure.figsize':(10,6)}, \n",
        "    style=\"white\"\n",
        ")"
      ],
      "metadata": {
        "id": "gJLJFz5w6ZGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noteevents = pd.read_csv(NOTEEVENTS_CSV, low_memory = False)"
      ],
      "metadata": {
        "id": "2Y3ANqgA6a-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noteevents_orig = noteevents"
      ],
      "metadata": {
        "id": "72Nmx87-8ASN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noteevents.info()"
      ],
      "metadata": {
        "id": "vj0T8qaK7T8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lns = [len(str(x)) for x in noteevents['TEXT']]\n",
        "sns.distplot(lns, kde=False, axlabel='Document length')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3UNivDrp6K3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort lengths\n",
        "lns.sort()\n",
        "# Take 5% as the removal size\n",
        "rm_size = int(len(lns) / 100) * 5\n",
        "\n",
        "# Now plot with removal of most/least frequent\n",
        "sns.distplot(lns[rm_size:-rm_size], kde=False, axlabel='Document length')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "464rkSp37sOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows from the dataframe based on document length, this is not really\n",
        "#straightforward, so we'll approximate it and find the document length that is used as a cutoff \n",
        "min_ln = max(lns[0:rm_size])\n",
        "max_ln = min(lns[-rm_size:])\n",
        "\n",
        "noteevents = noteevents[[True if len(str(x)) > min_ln and len(str(x)) < max_ln else False for x in noteevents['TEXT']]]\n",
        "noteevents.head()"
      ],
      "metadata": {
        "id": "ELZgxSKs8FjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length after cleaning : {len(noteevents)}\")\n",
        "print(f\"Length of the original: {len(noteevents_orig)}\")"
      ],
      "metadata": {
        "id": "racAh9DN8dRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(noteevents['SUBJECT_ID'].value_counts().values, kde=False, axlabel='Documents per patient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VgwnUfoz8uEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Again a bit of clean-up, let's remove the bottom/top 1% of patients based on the number of \n",
        "#documents they have. \n",
        "docs_per_pt = noteevents['SUBJECT_ID'].value_counts()\n",
        "docs_per_pt_vals = docs_per_pt.values\n",
        "docs_per_pt_vals.sort()\n",
        "docs_per_pt_vals"
      ],
      "metadata": {
        "id": "YvXzKzZHGVYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm_size = int(len(docs_per_pt_vals) / 100) * 1\n",
        "min_ln = max(docs_per_pt_vals[0:rm_size])\n",
        "max_ln = min(docs_per_pt_vals[-rm_size:])"
      ],
      "metadata": {
        "id": "d56rWU9_LRZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_ln"
      ],
      "metadata": {
        "id": "d-zHtkOUL8ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_subject_id = set([k for k, v in docs_per_pt.items() if v > 10 and v < 20])\n",
        "noteevents_rm_docs_per_pt = noteevents[[True if subject_id in keep_subject_id else False \n",
        "                  for subject_id in noteevents['SUBJECT_ID'].values]]\n",
        "noteevents_rm_docs_per_pt.head()"
      ],
      "metadata": {
        "id": "J8EY-BSI9GqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length after cleaning : {len(noteevents_rm_docs_per_pt)}\")\n",
        "print(f\"Length of the original: {len(noteevents_orig)}\")"
      ],
      "metadata": {
        "id": "TUFemXv19SQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(noteevents_rm_docs_per_pt['SUBJECT_ID'].value_counts().values, kde=False, axlabel='Documents per patient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ma-CENx19Zym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Sample of Data"
      ],
      "metadata": {
        "id": "whGYsIexrnwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjects = pd.DataFrame(keep_subject_id)"
      ],
      "metadata": {
        "id": "ZxpDyGojM5DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(subjects)"
      ],
      "metadata": {
        "id": "eCn1dhIgNXWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sample from subject ids \n",
        "sub_samples = subjects.sample(frac=0.01, random_state=4132023)"
      ],
      "metadata": {
        "id": "U4PJzM_AJ8Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sub_samples)"
      ],
      "metadata": {
        "id": "pc1MzJ8QC71p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_samp = sub_samples.iloc[:,0].values.tolist()"
      ],
      "metadata": {
        "id": "fEuP6TT4NQFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_notes = noteevents_rm_docs_per_pt[[True if subject_id in keep_samp else False \n",
        "                  for subject_id in noteevents_rm_docs_per_pt['SUBJECT_ID'].values]]"
      ],
      "metadata": {
        "id": "88GdUUdbrtgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APeyfexrDZAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(sampled_notes['SUBJECT_ID'].value_counts().values, kde=False, axlabel='Documents per patient')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nqpTmX9zAeSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_notes = sampled_notes.set_index(['ROW_ID'])"
      ],
      "metadata": {
        "id": "QQWBtsHcaO-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_notes.info()"
      ],
      "metadata": {
        "id": "sWW2SmgK0A0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_notes.to_csv('/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/Original-Data/SAMPLEDNOTEEVENTS.csv', index=False)"
      ],
      "metadata": {
        "id": "AwHj8qJRBht7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLEDNOTEEVENTS = '/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/Original-Data/SAMPLEDNOTEEVENTS.csv'"
      ],
      "metadata": {
        "id": "IS7K-SexCkOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(SAMPLEDNOTEEVENTS, low_memory=False)\n",
        "test.info()"
      ],
      "metadata": {
        "id": "FtUlZTPkGOiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Cleaning & DL Preparation"
      ],
      "metadata": {
        "id": "C-uH5wdUfi5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the noteEvents_preproc Python file"
      ],
      "metadata": {
        "id": "S2S-30AYARvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/CS598-DLH-Final-Project/patient_trajectory_prediction/data_cleaning/noteEvents_preproc.py"
      ],
      "metadata": {
        "id": "7ebxU1lkC0jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT = '/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/Output-Data/output.csv'"
      ],
      "metadata": {
        "id": "AZbGOR40HRFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputdata = pd.read_csv(OUTPUT, low_memory=False)"
      ],
      "metadata": {
        "id": "rgBVHtScFPjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputdata.reset_index(inplace=True)\n",
        "outputdata = outputdata.rename(columns = {'index':'new column name'})"
      ],
      "metadata": {
        "id": "qtf7u6DWHX2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputdata.head()"
      ],
      "metadata": {
        "id": "uSt21kFuooyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputdata.to_csv('/content/drive/MyDrive/CS598-DLH-Final-Project/QuickUMLS/data/chunkssmall/1.csv', index = False, header = False)"
      ],
      "metadata": {
        "id": "w1lQVfmYpQFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the quickUMLS_getCUI.py file"
      ],
      "metadata": {
        "id": "8-YuxvZ4ewtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/CS598-DLH-Final-Project/patient_trajectory_prediction/concept_annotation/quickUMLS_getCUI.py"
      ],
      "metadata": {
        "id": "ePSnOKFCtO5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the quickumls_processing.py file"
      ],
      "metadata": {
        "id": "yCN1aRIfe7Dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/CS598-DLH-Final-Project/patient_trajectory_prediction/concept_annotation/quickumls_processing.py /content/drive/MyDrive/CS598-DLH-Final-Project/QuickUMLS/data/outputchunkssmall/concatenated_output.csv"
      ],
      "metadata": {
        "id": "cv2wszxPKR21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/Output-Data/post_processed_output.csv', low_memory=False,header = None)\n",
        "test.iloc[:,2] = test.iloc[:,2].fillna(0)\n",
        "test.iloc[:,2] = test.iloc[:,2].astype(int)\n",
        "test.info()\n"
      ],
      "metadata": {
        "id": "xDyu8I0nbeoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.to_csv('/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/Output-Data/post_process_output_no_index.csv', header = False, index = False)"
      ],
      "metadata": {
        "id": "ODOU8ujp52wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the 01_data_prepartion.py file"
      ],
      "metadata": {
        "id": "zdZ3tavMfLTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/CS598-DLH-Final-Project/patient_trajectory_prediction/PyTorch_scripts/diagnoses_prediction/01_data_preparation.py"
      ],
      "metadata": {
        "id": "N6Ny6kIsy7D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Code, Evaluation Code, Pretrained Model"
      ],
      "metadata": {
        "id": "GC-am9tKfwmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/CS598-DLH-Final-Project/patient_trajectory_prediction/PyTorch_scripts/diagnoses_prediction/02_FFN_diagprediction.py"
      ],
      "metadata": {
        "id": "f7PXyW_a8Tn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/CS598-DLH-Final-Project/patient_trajectory_prediction/PyTorch_scripts/diagnoses_prediction/02_GRU_train_GPU.py"
      ],
      "metadata": {
        "id": "1OUtTAIwgchK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/CS598-DLH-Final-Project/patient_trajectory_prediction/PyTorch_scripts/diagnoses_prediction/03_GRU_test.py"
      ],
      "metadata": {
        "id": "6Zo2imJwjCOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ablation Study"
      ],
      "metadata": {
        "id": "cnI6CUZI-4pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arguments"
      ],
      "metadata": {
        "id": "O6da3ajkV1uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class args:\n",
        "  inputdata = '/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/Output-Data/prepared_data.npz'\n",
        "  outfile = '/content/drive/MyDrive/CS598-DLH-Final-Project/Data-Preprocessing/Output-Data/model_output_ablation.pt'\n",
        "  hiddenDimSize = 50\n",
        "  batchSize = 1000\n",
        "  nEpochs = 10000\n",
        "  lr = 0.01\n",
        "  dropOut = 0.5\n",
        "  kFold = 5\n",
        "  withCCS = 0\n",
        "\n",
        "ARGS = args()"
      ],
      "metadata": {
        "id": "oRxgv9nxWF93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        ARGS.inputdim = ARGS.numberOfInputCUIInts + ARGS.numberOfInputCCSInts if ARGS.withCCS else ARGS.numberOfInputCUIInts\n",
        "        self.hidden = nn.Linear(ARGS.inputdim, ARGS.hiddenDimSize)\n",
        "        self.hidden2 = nn.Linear(ARGS.hiddenDimSize, ARGS.numberOfOutputCodes)\n",
        "\n",
        "        # Define sigmoid activation and softmax output\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # DropOut\n",
        "        self.dropout = nn.Dropout(p=ARGS.dropOut)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.hidden(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.hidden2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class my_dataset(dt.Dataset):\n",
        "    def __init__(self, data, label):\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.label[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def load_tensors():\n",
        "    # -----------------------------------------\n",
        "    # pickle input map - each entry is a pair (subject_id, [(hadm_id,admittime, [CUIsvector], [CCSsvector])]\n",
        "    subjecttoadm_map = pickle.load(open(ARGS.inputdata, 'rb'))\n",
        "    setOfDistinctCUIs = set()\n",
        "    setOfDistinctCCSs = set()\n",
        "    cuitoint = dict()\n",
        "    ccstoint = dict()\n",
        "    for subject in subjecttoadm_map.keys():\n",
        "        patientData = subjecttoadm_map[subject]\n",
        "        for ithAdmis in patientData:\n",
        "            for CUIcode in ithAdmis[2]:\n",
        "                setOfDistinctCUIs.add(CUIcode)\n",
        "            for CCScode in ithAdmis[3]:\n",
        "                setOfDistinctCCSs.add(CCScode)\n",
        "    for i, cui in enumerate(setOfDistinctCUIs):\n",
        "        cuitoint[cui] = i\n",
        "    for i, ccs in enumerate(setOfDistinctCCSs):\n",
        "        ccstoint[ccs] = i\n",
        "    print(\"-> \" + str(\n",
        "        len(subjecttoadm_map)) + \" patients' CUI notes and CCS codes at dimension 0 for file: \" + ARGS.inputdata)\n",
        "    ARGS.numberOfInputCUIInts = len(setOfDistinctCUIs)\n",
        "    ARGS.numberOfInputCCSInts = len(setOfDistinctCCSs)\n",
        "    # -------------------------------------------\n",
        "    ARGS.numberOfOutputCodes = len(setOfDistinctCCSs)\n",
        "    print('Remaining patients:', len(subjecttoadm_map))\n",
        "    # Convert everything to list of list of list (patient x admission x CUInote_vector/diagnoses to ease the manipulation in batches\n",
        "    vectors_trainListX = []\n",
        "    diagnoses_trainListY = []\n",
        "    # hadm_id_List = []\n",
        "    for pID, adList in subjecttoadm_map.items():\n",
        "        for i, adm in enumerate(adList):\n",
        "            # hadm_id_List.append(adm[0])\n",
        "            if i + 1 == len(adList):\n",
        "                # Avoid adding the last admission in X\n",
        "                one_hot_CCS = [0] * ARGS.numberOfInputCCSInts\n",
        "                for ccs_int in adm[3]:\n",
        "                    one_hot_CCS[ccstoint[ccs_int]] = 1\n",
        "                diagnoses_trainListY.append(one_hot_CCS)\n",
        "                continue\n",
        "            one_hot_CUI = [0] * ARGS.numberOfInputCUIInts\n",
        "            one_hot_CCS = [0] * ARGS.numberOfInputCCSInts\n",
        "            for cui_int in adm[2]:\n",
        "                one_hot_CUI[cuitoint[cui_int]] = 1\n",
        "            for ccs_int in adm[3]:\n",
        "                one_hot_CCS[ccstoint[ccs_int]] = 1\n",
        "            one_hot_X = one_hot_CUI + one_hot_CCS if ARGS.withCCS else one_hot_CUI\n",
        "            vectors_trainListX.append(one_hot_X)\n",
        "            if i != 0:\n",
        "                # Add every admission diagnoses in Y but the first one's diagnoses\n",
        "                diagnoses_trainListY.append(one_hot_CCS)\n",
        "\n",
        "    mapIndexPosition = list(zip(vectors_trainListX, diagnoses_trainListY))\n",
        "    random.shuffle(mapIndexPosition)\n",
        "    vectors_trainListX, diagnoses_trainListY = zip(*mapIndexPosition)\n",
        "    return vectors_trainListX, diagnoses_trainListY\n",
        "\n",
        "\n",
        "def split(a, n):\n",
        "    k, m = divmod(len(a), n)\n",
        "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "def train():\n",
        "    data_x, data_y = load_tensors()\n",
        "\n",
        "    sizedata = len(data_x)\n",
        "    print(\"Data of size:\", sizedata)\n",
        "    # Split dataset into 5 sub-datasets\n",
        "    splitted_x = list(split(data_x, 5))\n",
        "    splitted_y = list(split(data_y, 5))\n",
        "    print(\"Available GPU :\", torch.cuda.is_available())\n",
        "    torch.cuda.set_device(0)\n",
        "    k = ARGS.kFold\n",
        "\n",
        "    # Prepare array of scores\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    # valloss_list = []\n",
        "    AUC_list = []\n",
        "    for ind_i in range(0,k):\n",
        "        # Prepare X_train Y_train X_test Y_test\n",
        "        X_test = splitted_x[ind_i]\n",
        "        Y_test = splitted_y[ind_i]\n",
        "        # Deep copy, otherwise iteration problem\n",
        "        copysplitX = list(splitted_x)\n",
        "        copysplitY = list(splitted_y)\n",
        "        del copysplitX[ind_i]\n",
        "        del copysplitY[ind_i]\n",
        "        X_train = copysplitX\n",
        "        Y_train = copysplitY\n",
        "        model = Network().cuda()\n",
        "        # XAVIER Init\n",
        "        model.apply(init_weights)\n",
        "        with torch.cuda.device(0):\n",
        "            # Hyperparameters :\n",
        "            epochs = ARGS.nEpochs\n",
        "            batchsize = ARGS.batchSize\n",
        "            learning_rate = ARGS.lr\n",
        "            log_interval = 2\n",
        "            criterion = nn.BCEWithLogitsLoss()\n",
        "            # criterion = nn.BCELoss()\n",
        "            # criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "            # optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            # Train loader\n",
        "            numplist = np.array(X_train)\n",
        "            arrX = np.concatenate(numplist).tolist()\n",
        "            tensor_x = torch.Tensor(arrX).cuda()\n",
        "            numplist = np.array(Y_train)\n",
        "            arrY = np.concatenate(numplist).tolist()\n",
        "            tensor_y = torch.Tensor(arrY).cuda()\n",
        "            print(\"Shape X:\", np.shape(arrX), \"Shape Y:\", np.shape(arrY))\n",
        "            # tensor_x = torch.Tensor(np.array(X_train).tolist()).cuda()  # transform to torch tensor\n",
        "            # tensor_y = torch.Tensor(np.array(Y_train).tolist()).cuda()\n",
        "            dataset = dt.TensorDataset(tensor_x, tensor_y)  # create your dataset\n",
        "            # train_size = int(len(dataset))\n",
        "            # print(\"train_size =\", train_size)\n",
        "            train_loader = dt.DataLoader(\n",
        "                dataset,\n",
        "                batch_size=batchsize,\n",
        "                shuffle=True)\n",
        "\n",
        "            # Test loader\n",
        "            tensor_x = torch.Tensor(np.array(X_test).tolist()).cuda()  # transform to torch tensor\n",
        "            tensor_y = torch.Tensor(np.array(Y_test).tolist()).cuda()\n",
        "            dataset = dt.TensorDataset(tensor_x, tensor_y)  # create your dataset\n",
        "            test_loader = dt.DataLoader(\n",
        "                dataset,\n",
        "                batch_size=batchsize,\n",
        "                shuffle=True)\n",
        "\n",
        "            # Training\n",
        "            for epoch in range(epochs):\n",
        "                for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                    data, target = Variable(data), Variable(target)\n",
        "                    optimizer.zero_grad()\n",
        "                    net_out = model(data)\n",
        "                    loss = criterion(net_out, target)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if batch_idx % log_interval == 0:\n",
        "                        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: '.format(\n",
        "                            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                                   100. * batch_idx / len(train_loader)))\n",
        "                        print(loss.data)\n",
        "\n",
        "            # saving model\n",
        "            # torch.save(model.state_dict(), ARGS.outFile + str(ind_i))\n",
        "\n",
        "            # Testing and save score\n",
        "            total = 0\n",
        "            correct = 0\n",
        "            model.eval()\n",
        "            # Validation loss\n",
        "            # loss_values = []\n",
        "            itr_ctr = 0\n",
        "            for batch_idx, (data, target) in enumerate(test_loader):\n",
        "                #with torch.no_grad():\n",
        "                itr_ctr += 1\n",
        "                data, target = Variable(data, volatile=True), Variable(target, volatile=True)\n",
        "                net_out = model(data)\n",
        "                loss = criterion(net_out, target)\n",
        "                # loss_values.append(loss)\n",
        "\n",
        "            # Validation Loss in the list\n",
        "            # valloss_list.append(np.mean(loss_values))\n",
        "\n",
        "            P = list()\n",
        "            R = list()\n",
        "            # Precisions\n",
        "            for i in range(1,4):\n",
        "                for data in test_loader:\n",
        "                    x, labels = data\n",
        "                    outputs = model(Variable(x)).detach() # output is a tensor of size [BATCHSIZE][ARGS.numberOfOutputCodes]\n",
        "                    _, predicted = torch.topk(outputs.data, i)\n",
        "                    for y_predlist, y in zip(predicted, labels):\n",
        "                        for y_pred in y_predlist:\n",
        "                            total += 1\n",
        "                            if y[y_pred] == 1:\n",
        "                                correct += 1\n",
        "\n",
        "                precision = correct / total\n",
        "                P.append(precision)\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "            # Number of diagnostic for each sample (mean of 12 codes, max of 30 codes, R@10 - R@20 - R@30 seems appropriate)\n",
        "            total_true_list = list()\n",
        "            for data in test_loader:\n",
        "                x, labels = data\n",
        "                for y in labels :\n",
        "                    total_true = 0\n",
        "                    for val in y :\n",
        "                        if val == 1:\n",
        "                            total_true += 1\n",
        "                    total_true_list.append(total_true)\n",
        "\n",
        "            # Recalls\n",
        "            for i in range(10,40,10):\n",
        "                total_true_list_cpy = list(total_true_list)\n",
        "                for data in test_loader:\n",
        "                    x, labels = data\n",
        "                    outputs = model(Variable(x)).detach()\n",
        "                    _, predicted = torch.topk(outputs.data, i)\n",
        "                    for y_predlist, y in zip(predicted, labels):\n",
        "                        total += total_true_list_cpy.pop(0)\n",
        "                        for y_pred in y_predlist:\n",
        "                            if y[y_pred] == 1:\n",
        "                                correct += 1\n",
        "\n",
        "                recall = correct / total\n",
        "                R.append(recall)\n",
        "                correct = 0\n",
        "                total = 0\n",
        "            precision_list.append(P)\n",
        "            recall_list.append(R)\n",
        "\n",
        "            # AUROC\n",
        "            YTRUE = None\n",
        "            YPROBA = None\n",
        "            for data in test_loader:\n",
        "                x, labels = data\n",
        "                x, labels = Variable(x), Variable(labels)\n",
        "                outputs = model(x).detach().cpu().numpy()\n",
        "                labels = labels.detach().cpu().numpy()\n",
        "                for batch_true, batch_prob in zip(labels, outputs):\n",
        "                    YTRUE = np.concatenate((YTRUE, [batch_true]), axis=0) if YTRUE is not None else [batch_true]\n",
        "                    YPROBA = np.concatenate((YPROBA, [batch_prob]), axis=0) if YPROBA is not None else [batch_prob]\n",
        "            ROC_avg_score=roc(YTRUE, YPROBA, average='micro', multi_class='ovr')\n",
        "            AUC_list.append(ROC_avg_score)\n",
        "\n",
        "    # Output score of each fold + average\n",
        "    print(\"Scores for each fold:\")\n",
        "    print(\"Precision:\", precision_list)\n",
        "    print(\"Recall:\", recall_list)\n",
        "    print(\"AUROC:\", AUC_list)\n",
        "    # print(\"Loss:\", valloss_list)\n",
        "    print(\"Average scores:\")\n",
        "    P1=(sum([precision_list[k][0] for k in range(0, k)])/k)\n",
        "    P2=(sum([precision_list[k][1] for k in range(0, k)])/k)\n",
        "    P3=(sum([precision_list[k][2] for k in range(0, k)])/k)\n",
        "    R10=(sum([recall_list[k][0] for k in range(0, k)])/k)\n",
        "    R20=(sum([recall_list[k][1] for k in range(0, k)])/k)\n",
        "    R30=(sum([recall_list[k][2] for k in range(0, k)])/k)\n",
        "    AUROC=(sum([AUC_list[k] for k in range(0,k)])/k)\n",
        "    # loss_avg=sum(valloss_list)/len(valloss_list)\n",
        "    print(\"Precision@1:\", P1)\n",
        "    print(\"Precision@2:\", P2)\n",
        "    print(\"Precision@3:\", P3)\n",
        "    print(\"Recall@10:\", R10)\n",
        "    print(\"Recall@20:\", R20)\n",
        "    print(\"Recall@30:\", R30)\n",
        "    print(\"AUROC:\", AUROC)\n",
        "    # print(\"Loss:\", loss_avg)\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "id": "Hs4_ZRVvLw67"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}